{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32d8a50",
   "metadata": {},
   "source": [
    "# Space Invaders with Deep Q-Networks\n",
    "\n",
    "In this notebook I will try to create a DQN that surpasses human ability in playing Space Invaders.\n",
    "Lets see how it goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9de80dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ale-py\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdcb81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c797c6",
   "metadata": {},
   "source": [
    "## SCORING in Space invaders \n",
    "\n",
    "The SPACE INVADERS are worth 5, 10, 15, 20, 25, 30 points in\n",
    "the first through sixth rows respectively. (See diagram.) The\n",
    "point value of each target stays the same as it drops lower on\n",
    "the screen. Each complete set of SPACE INVADERS is worth 630\n",
    "points.\n",
    "\n",
    "\n",
    "taken from https://atariage.com/manual_html_page.php?SoftwareLabelID=460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76254b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 episodes\n",
      "Videos will be recorded every 250 episodes\n",
      "Videos saved to: space_invaders/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n",
      "/home/f4zination/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/f4zination/development/DQN-SpaceInvaders/space_invaders folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import ale_py\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Training configuration\n",
    "training_period = 250           # Record video every 250 episodes\n",
    "num_training_episodes = 5_000  # Total training episodes\n",
    "env_name = \"ALE/SpaceInvaders-v5\" # has a default obs tpye of rgb, 4 frames are skipped and the repeat action propability is 0.25\n",
    "\n",
    "# Set up logging for episode statistics\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Create environment with periodic video recording\n",
    "# possible to activate full action space with full_action_space=True\n",
    "# using grayscaling to reduce input of Q-Network\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "\n",
    "# Record videos periodically (every 250 episodes)\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"space_invaders\",\n",
    "    name_prefix=\"training\",\n",
    "    episode_trigger=lambda x: x % training_period == 0  # Only record every 250th episode\n",
    ")\n",
    "\n",
    "# Track statistics for every episode (lightweight)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "\n",
    "print(f\"Training for {num_training_episodes} episodes\")\n",
    "print(f\"Videos will be recorded every {training_period} episodes\")\n",
    "print(f\"Videos saved to: space_invaders/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10f824d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import FrameStackObservation\n",
    "FRAME_STACK_SIZE = 4\n",
    "\n",
    "\n",
    "stacked_env = FrameStackObservation(env, stack_size=FRAME_STACK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb53c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        action_space\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=8, stride=4 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        dummy = torch.zeros(1, in_channels, 210, 160)\n",
    "        dummy_out = self.conv_stack(dummy)\n",
    "        self.num_features = dummy_out.numel()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.num_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, action_space)\n",
    "        )\n",
    "\n",
    "        self.model = self.conv_stack.extend(self.fc)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39632d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68ec6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpaceInvaderAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "        frame_stacking: int = FRAME_STACK_SIZE,\n",
    "    ):\n",
    "        self.env = env\n",
    "        \n",
    "        # Q-Network to represent the current policy\n",
    "        self.policy_network = QNetwork(in_channels=frame_stacking, action_space=env.action_space.n)\n",
    "\n",
    "        \n",
    "        self.target_network = QNetwork(in_channels=frame_stacking, action_space=env.action_space.n)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.discount_factor = discount_factor  # How much we care about future rewards also known as gamma\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # initialize a replay buffer\n",
    "        self.memory = ReplayMemory(10_000)\n",
    "        self.batch_size = 120\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error = []\n",
    "\n",
    "    # currently using a random action\n",
    "    def get_action(self, obs)-> int:\n",
    "        # With probability epsilon: explore (random action)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        # With probability (1-epsilon): exploit (best known action)\n",
    "        else:\n",
    "            obs = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.policy_network(obs)).item()\n",
    "\n",
    "        \n",
    "    def update(self):\n",
    "        if len(self.memory) < 5000:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = zip(*batch)\n",
    "\n",
    "        state_batch = torch.FloatTensor(state_batch)\n",
    "        action_batch = torch.LongTensor(action_batch).unsqueeze(1)\n",
    "        reward_batch = torch.FloatTensor(reward_batch)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch)\n",
    "\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.policy_network(state_batch).gather(1, action_batch).squeeze().to(device)\n",
    "\n",
    "        # Compute target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_network(next_state_batch).max(1)[0]\n",
    "            target_q_values = reward_batch + self.discount_factor * max_next_q_values * (1 - done_batch)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc40e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 100_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "final_epsilon = 0.1         # Always keep some exploration\n",
    "\n",
    "\n",
    "agent = SpaceInvaderAgent(\n",
    "    env=stacked_env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa29fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27794/1625775416.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  state_batch = torch.FloatTensor(state_batch)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 4, 8, 8], expected input[1, 120, 210, 160] to have 4 channels, but got 120 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m step_counter += \u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m agent.memory.push(state, action, reward, next_state)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m state = next_state\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step_counter % \u001b[32m10000\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mSpaceInvaderAgent.update\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m next_state_batch = torch.FloatTensor(next_state_batch)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Compute Q-values for current states\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m.gather(\u001b[32m1\u001b[39m, action_batch).squeeze().to(device)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Compute target Q-values using the target network\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mQNetwork.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/development/DQN-SpaceInvaders/.venv/lib/python3.13/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Given groups=1, weight of size [16, 4, 8, 8], expected input[1, 120, 210, 160] to have 4 channels, but got 120 channels instead"
     ]
    }
   ],
   "source": [
    "for episode in range(n_episodes):\n",
    "    state, info = env.reset()\n",
    "    step_counter = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        step_counter += 1\n",
    "        agent.memory.push(state, action, reward, next_state)\n",
    "\n",
    "        agent.update()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if step_counter % 10000 == 0:\n",
    "            agent.target_network.load_state_dict(agent.policy_network.state_dict())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(agent, env):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # --- Plot 1: Episode Rewards ---\n",
    "    # RecordEpisodeStatistics stores rewards in env.return_queue\n",
    "    if hasattr(env, 'return_queue') and len(env.return_queue) > 0:\n",
    "        rewards = list(env.return_queue)\n",
    "        ax1.plot(rewards, label='Reward per Episode', alpha=0.3, color='blue')\n",
    "        \n",
    "        # Add a rolling average to see the trend through the noise\n",
    "        if len(rewards) >= 10:\n",
    "            rolling_avg = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "            ax1.plot(rolling_avg, label='Rolling Avg (10 ep)', color='darkblue')\n",
    "            \n",
    "        ax1.set_title(\"Episode Rewards\")\n",
    "        ax1.set_xlabel(\"Episode\")\n",
    "        ax1.set_ylabel(\"Total Reward\")\n",
    "        ax1.legend()\n",
    "\n",
    "    # --- Plot 2: Training Loss ---\n",
    "    if len(agent.training_error) > 0:\n",
    "        # RL loss is often very \"spiky\", so we plot it on a log scale or with transparency\n",
    "        ax2.plot(agent.training_error, label='Loss', alpha=0.3, color='orange')\n",
    "        \n",
    "        # Add rolling average for loss\n",
    "        if len(agent.training_error) >= 50:\n",
    "            rolling_loss = np.convolve(agent.training_error, np.ones(50)/50, mode='valid')\n",
    "            ax2.plot(rolling_loss, label='Rolling Avg (50 updates)', color='red')\n",
    "            \n",
    "        ax2.set_title(\"Training Loss\")\n",
    "        ax2.set_xlabel(\"Update Step\")\n",
    "        ax2.set_ylabel(\"Loss (MSE)\")\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No loss data yet.\\nDid you remember to append to training_error?\", \n",
    "                 ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call this after your training loop or inside the loop every X episodes\n",
    "plot_training_results(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
