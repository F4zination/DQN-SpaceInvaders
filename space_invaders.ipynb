{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32d8a50",
   "metadata": {},
   "source": [
    "# Space Invaders with Deep Q-Networks\n",
    "\n",
    "In this notebook I will try to create a DQN that surpasses human ability in playing Space Invaders.\n",
    "Lets see how it goes..."
   ]
  },
  {
   "cell_type": "code",
   "id": "9de80dd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:01.787057Z",
     "start_time": "2025-12-18T19:25:01.781910Z"
    }
   },
   "source": [
    "#!pip install ale-py\n",
    "#!pip uninstall torch torchvision -y\n",
    "#!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130\n",
    "#!pip install gymnasium[atari]\n",
    "#!pip install matplotlib"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "8cdcb81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:03.712641Z",
     "start_time": "2025-12-18T19:25:01.810044Z"
    }
   },
   "source": [
    "import torch\n",
    "print(\"CUDA\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "32c797c6",
   "metadata": {},
   "source": [
    "## SCORING in Space invaders \n",
    "\n",
    "The SPACE INVADERS are worth 5, 10, 15, 20, 25, 30 points in\n",
    "the first through sixth rows respectively. (See diagram.) The\n",
    "point value of each target stays the same as it drops lower on\n",
    "the screen. Each complete set of SPACE INVADERS is worth 630\n",
    "points.\n",
    "\n",
    "\n",
    "taken from https://atariage.com/manual_html_page.php?SoftwareLabelID=460"
   ]
  },
  {
   "cell_type": "code",
   "id": "76254b8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:04.076611Z",
     "start_time": "2025-12-18T19:25:03.803827Z"
    }
   },
   "source": [
    "import logging\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo\n",
    "import ale_py\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "# Training configuration\n",
    "training_period = 250           # Record video every 250 episodes\n",
    "num_training_episodes = 5_000  # Total training episodes\n",
    "env_name = \"ALE/SpaceInvaders-v5\" # has a default obs tpye of rgb, 4 frames are skipped and the repeat action propability is 0.25\n",
    "\n",
    "# Set up logging for episode statistics\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "# Create environment with periodic video recording\n",
    "# possible to activate full action space with full_action_space=True\n",
    "# using grayscaling to reduce input of Q-Network\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\", obs_type=\"grayscale\")\n",
    "\n",
    "# Record videos periodically (every 250 episodes)\n",
    "env = RecordVideo(\n",
    "    env,\n",
    "    video_folder=\"space_invaders\",\n",
    "    name_prefix=\"training\",\n",
    "    episode_trigger=lambda x: x % training_period == 0  # Only record every 250th episode\n",
    ")\n",
    "\n",
    "# Track statistics for every episode (lightweight)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "\n",
    "print(f\"Training for {num_training_episodes} episodes\")\n",
    "print(f\"Videos will be recorded every {training_period} episodes\")\n",
    "print(f\"Videos saved to: space_invaders/\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 episodes\n",
      "Videos will be recorded every 250 episodes\n",
      "Videos saved to: space_invaders/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\finnr\\PycharmProjects\\DQN-SpaceInvaders\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001B[33mWARN: Overwriting existing videos at C:\\Users\\finnr\\PycharmProjects\\DQN-SpaceInvaders\\space_invaders folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "10f824d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:04.091868Z",
     "start_time": "2025-12-18T19:25:04.084506Z"
    }
   },
   "source": [
    "from gymnasium.wrappers import FrameStackObservation\n",
    "FRAME_STACK_SIZE = 4\n",
    "\n",
    "\n",
    "stacked_env = FrameStackObservation(env, stack_size=FRAME_STACK_SIZE)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "fb53c179",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:04.111042Z",
     "start_time": "2025-12-18T19:25:04.106327Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        action_space\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=8, stride=4 ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Infer fully-connected input size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, in_channels, 210, 160)\n",
    "            dummy_out = self.conv_stack(dummy)\n",
    "            self.num_features = dummy_out.shape[1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.num_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "39632d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:04.120500Z",
     "start_time": "2025-12-18T19:25:04.116562Z"
    }
   },
   "source": [
    "from collections import namedtuple, deque\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "68ec6560",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:04.134525Z",
     "start_time": "2025-12-18T19:25:04.126120Z"
    }
   },
   "source": [
    "\n",
    "class SpaceInvaderAgent:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "        frame_stacking: int = FRAME_STACK_SIZE,\n",
    "        device: str | None = None,\n",
    "    ):\n",
    "        self.env = env\n",
    "\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Q-Network to represent the current policy\n",
    "        self.policy_network = QNetwork(in_channels=frame_stacking, action_space=env.action_space.n).to(self.device)\n",
    "\n",
    "        self.target_network = QNetwork(in_channels=frame_stacking, action_space=env.action_space.n).to(self.device)\n",
    "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.discount_factor = discount_factor  # How much we care about future rewards also known as gamma\n",
    "\n",
    "        # Exploration parameters\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # initialize a replay buffer\n",
    "        self.memory = ReplayMemory(10_000)\n",
    "        self.batch_size = 120\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error = []\n",
    "\n",
    "    def _obs_to_tensor(self, obs) -> torch.Tensor:\n",
    "        \"\"\"Convert a single stacked observation to a float32 tensor in NCHW.\"\"\"\n",
    "        obs = np.array(obs)  # handles FrameStackObservation / LazyFrames\n",
    "        # Expected shapes:\n",
    "        # - channels-first: (stack, H, W)\n",
    "        # - channels-last:  (H, W, stack)\n",
    "        if obs.ndim != 3:\n",
    "            raise ValueError(f\"Expected 3D obs (stack,H,W) or (H,W,stack), got shape {obs.shape}\")\n",
    "\n",
    "        if obs.shape[0] == FRAME_STACK_SIZE:\n",
    "            # (stack, H, W)\n",
    "            pass\n",
    "        elif obs.shape[-1] == FRAME_STACK_SIZE:\n",
    "            # (H, W, stack) -> (stack, H, W)\n",
    "            obs = np.moveaxis(obs, -1, 0)\n",
    "        else:\n",
    "            raise ValueError(f\"Can't infer channel dimension from obs shape {obs.shape}\")\n",
    "\n",
    "        return torch.from_numpy(obs).to(dtype=torch.float32, device=self.device)\n",
    "\n",
    "    def get_action(self, obs) -> int:\n",
    "        # With probability epsilon: explore (random action)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        # With probability (1-epsilon): exploit (best known action)\n",
    "        obs_t = self._obs_to_tensor(obs).unsqueeze(0)  # (1, C, H, W)\n",
    "        with torch.no_grad():\n",
    "            q = self.policy_network(obs_t)\n",
    "            return int(torch.argmax(q, dim=1).item())\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.memory) < 5000:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        state_batch, action_batch, next_state_batch, reward_batch, done_batch = zip(*batch)\n",
    "\n",
    "        # Convert each observation robustly to (C,H,W) then stack -> (B,C,H,W)\n",
    "        state_batch = torch.stack([self._obs_to_tensor(s) for s in state_batch], dim=0)\n",
    "        next_state_batch = torch.stack([self._obs_to_tensor(s) for s in next_state_batch], dim=0)\n",
    "\n",
    "        action_batch = torch.as_tensor(action_batch, dtype=torch.int64, device=self.device).unsqueeze(1)\n",
    "        reward_batch = torch.as_tensor(reward_batch, dtype=torch.float32, device=self.device)\n",
    "        done_batch = torch.as_tensor(done_batch, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.policy_network(state_batch).gather(1, action_batch).squeeze(1)\n",
    "\n",
    "        # Compute target Q-values using the target network\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_network(next_state_batch).max(1)[0]\n",
    "            target_q_values = reward_batch + self.discount_factor * max_next_q_values * (1.0 - done_batch)\n",
    "\n",
    "        loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.training_error.append(float(loss.item()))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduce exploration rate after each episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "4cc40e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-18T19:25:05.836752Z",
     "start_time": "2025-12-18T19:25:04.139626Z"
    }
   },
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 0.01        # How fast to learn (higher = faster but less stable)\n",
    "n_episodes = 100_000        # Number of hands to practice\n",
    "start_epsilon = 1.0         # Start with 100% random actions\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # Reduce exploration over time\n",
    "final_epsilon = 0.1         # Always keep some exploration\n",
    "\n",
    "\n",
    "agent = SpaceInvaderAgent(\n",
    "    env=stacked_env,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "aa29fe28",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-18T19:25:05.875129Z"
    }
   },
   "source": [
    "for episode in range(n_episodes):\n",
    "    # NOTE: use stacked_env consistently (not the unstacked env)\n",
    "    state, info = stacked_env.reset()\n",
    "    step_counter = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, terminated, truncated, info = stacked_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        step_counter += 1\n",
    "        agent.memory.push(state, action, next_state, reward, float(done))\n",
    "\n",
    "        agent.update()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if step_counter % 10000 == 0:\n",
    "            agent.target_network.load_state_dict(agent.policy_network.state_dict())\n",
    "            agent.target_network.eval()\n",
    "            print(f\"Episode {episode} - Step {step_counter}: Updated target network.\")\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(agent, env):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # --- Plot 1: Episode Rewards ---\n",
    "    # RecordEpisodeStatistics stores rewards in env.return_queue\n",
    "    if hasattr(env, 'return_queue') and len(env.return_queue) > 0:\n",
    "        rewards = list(env.return_queue)\n",
    "        ax1.plot(rewards, label='Reward per Episode', alpha=0.3, color='blue')\n",
    "        \n",
    "        # Add a rolling average to see the trend through the noise\n",
    "        if len(rewards) >= 10:\n",
    "            rolling_avg = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
    "            ax1.plot(rolling_avg, label='Rolling Avg (10 ep)', color='darkblue')\n",
    "            \n",
    "        ax1.set_title(\"Episode Rewards\")\n",
    "        ax1.set_xlabel(\"Episode\")\n",
    "        ax1.set_ylabel(\"Total Reward\")\n",
    "        ax1.legend()\n",
    "\n",
    "    # --- Plot 2: Training Loss ---\n",
    "    if len(agent.training_error) > 0:\n",
    "        # RL loss is often very \"spiky\", so we plot it on a log scale or with transparency\n",
    "        ax2.plot(agent.training_error, label='Loss', alpha=0.3, color='orange')\n",
    "        \n",
    "        # Add rolling average for loss\n",
    "        if len(agent.training_error) >= 50:\n",
    "            rolling_loss = np.convolve(agent.training_error, np.ones(50)/50, mode='valid')\n",
    "            ax2.plot(rolling_loss, label='Rolling Avg (50 updates)', color='red')\n",
    "            \n",
    "        ax2.set_title(\"Training Loss\")\n",
    "        ax2.set_xlabel(\"Update Step\")\n",
    "        ax2.set_ylabel(\"Loss (MSE)\")\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No loss data yet.\\nDid you remember to append to training_error?\", \n",
    "                 ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call this after your training loop or inside the loop every X episodes\n",
    "plot_training_results(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
